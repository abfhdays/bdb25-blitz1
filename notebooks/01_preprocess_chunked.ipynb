{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vLIVGV5ZJpW_"},"outputs":[],"source":["def rotate_direction_and_orientation(df):\n","\n","  \"\"\"\n","  Rotate the direction and orientation angles so that 0° points from left to right on the field, and increasing angle goes counterclockwise\n","  This should be done BEFORE the call to make_plays_left_to_right, because that function with compensate for the flipped angles.\n","\n","  :param df: the aggregate dataframe created using the aggregate_data() method\n","\n","  :return df: the aggregate dataframe with orientation and direction angles rotated 90° clockwise\n","  \"\"\"\n","\n","  df[\"o_clean\"] = (-(df[\"o\"] - 90)) % 360\n","  df[\"dir_clean\"] = (-(df[\"dir\"] - 90)) % 360\n","\n","  return df\n","\n","\n","def make_plays_left_to_right(df):\n","\n","  \"\"\"\n","  Flip tracking data so that all plays run from left to right. The new x, y, s, a, dis, o, and dir data\n","  will be stored in new columns with the suffix \"_clean\" even if the variables do not change from their original value.\n","\n","  :param df: the aggregate dataframe created using the aggregate_data() method\n","\n","  :return df: the aggregate dataframe with the new columns such that all plays run left to right\n","  \"\"\"\n","\n","  df[\"x_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\",\n","      120 - df[\"x\"],\n","      df[\n","          \"x\"\n","      ],  # 120 because the endzones (10 yds each) are included in the [\"x\"] values\n","  )\n","\n","  df[\"y_clean\"] = df[\"y\"]\n","  df[\"s_clean\"] = df[\"s\"]\n","  df[\"a_clean\"] = df[\"a\"]\n","  df[\"dis_clean\"] = df[\"dis\"]\n","\n","  df[\"o_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\", 180 - df[\"o_clean\"], df[\"o_clean\"]\n","  )\n","\n","  df[\"o_clean\"] = (df[\"o_clean\"] + 360) % 360  # remove negative angles\n","\n","  df[\"dir_clean\"] = np.where(\n","      df[\"playDirection\"] == \"left\", 180 - df[\"dir_clean\"], df[\"dir_clean\"]\n","  )\n","\n","  df[\"dir_clean\"] = (df[\"dir_clean\"] + 360) % 360  # remove negative angles\n","\n","  return df\n","\n","\n","import numpy as np\n","\n","def calculate_velocity_components(df):\n","    \"\"\"\n","    Calculate the velocity components (v_x and v_y) for each row in the dataframe.\n","\n","    :param df: the aggregate dataframe with \"_clean\" columns created using make_plays_left_to_right()\n","\n","    :return df: the dataframe with additional columns 'v_x' and 'v_y' representing the velocity components\n","    \"\"\"\n","\n","    df[\"dir_radians\"] = np.radians(df[\"dir_clean\"])\n","\n","    df[\"v_x\"] = df[\"s_clean\"] * np.cos(df[\"dir_radians\"])\n","    df[\"v_y\"] = df[\"s_clean\"] * np.sin(df[\"dir_radians\"])\n","\n","\n","    return df\n","\n","\n","def label_offense_defense_coverage(presnap_df, plays_df):\n","\n","  coverage_replacements = {\n","    'Cover-3 Cloud Right': 'Cover-3',\n","    'Cover-3 Cloud Left': 'Cover-3',\n","    'Cover-3 Seam': 'Cover-3',\n","    'Cover-3 Double Cloud': 'Cover-3',\n","    'Cover-6 Right': 'Cover-6',\n","    'Cover 6-Left': 'Cover-6',\n","    'Cover-1 Double': 'Cover-1'}\n","\n","  values_to_drop = [\"Miscellaneous\", \"Bracket\", \"Prevent\", \"Red Zone\", \"Goal Line\"]\n","\n","  plays_df['pff_passCoverage'] = plays_df['pff_passCoverage'].replace(coverage_replacements)\n","\n","  plays_df = plays_df.dropna(subset=['pff_passCoverage'])\n","  plays_df = plays_df[~plays_df['pff_passCoverage'].isin(values_to_drop)]\n","\n","  coverage_mapping = {\n","      'Cover-0': 0,\n","      'Cover-1': 1,\n","      'Cover-2': 2,\n","      'Cover-3': 3,\n","      'Quarters': 4,\n","      '2-Man': 5,\n","      'Cover-6': 6\n","  }\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'pff_passCoverage']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['pff_passCoverage'] = merged_df['pff_passCoverage'].map(coverage_mapping)\n","  merged_df.dropna(subset=['pff_passCoverage'], inplace=True)\n","\n","  return merged_df\n","\n","\n","def label_offense_defense_manzone(presnap_df, plays_df):\n","\n","  plays_df = plays_df.dropna(subset=['pff_manZone'])\n","\n","  coverage_mapping = {\n","      'Zone': 0,\n","      'Man': 1}\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'pff_manZone']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['pff_manZone'] = merged_df['pff_manZone'].map(coverage_mapping)\n","  merged_df.dropna(subset=['pff_manZone'], inplace=True)\n","\n","  return merged_df\n","\n","\n","def label_offense_defense_formation(presnap_df, plays_df):\n","\n","  \"\"\"\n","  Adds 'offense' and 'defense' columns to presnap_df, marking players as offense (1) or defense (0)\n","  based on possession team and defensive team from plays_df. Enumerates offensive formations\n","  and removes rows with missing formations.\n","\n","  Parameters:\n","  presnap_df (pd.DataFrame): DataFrame containing tracking data with 'gameId', 'playId', and 'club'.\n","  plays_df (pd.DataFrame): DataFrame containing 'gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'offenseFormation'.\n","\n","  Returns:\n","  pd.DataFrame: Updated presnap_df with added 'offense', 'defense', and enumerated 'offenseFormation' columns, with NaN formations dropped.\n","  \"\"\"\n","\n","  formation_mapping = {\n","      'EMPTY': 0,\n","      'I_FORM': 1,\n","      'JUMBO': 2,\n","      'PISTOL': 3,\n","      'SHOTGUN': 4,\n","      'SINGLEBACK': 5,\n","      'WILDCAT': 6\n","  }\n","\n","  merged_df = presnap_df.merge(\n","      plays_df[['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'offenseFormation']],\n","      on=['gameId', 'playId'],\n","      how='left'\n","  )\n","\n","  merged_df['defense'] = ((merged_df['club'] == merged_df['defensiveTeam']) & (merged_df['club'] != 'football')).astype(int)\n","\n","  merged_df['offenseFormation'] = merged_df['offenseFormation'].map(formation_mapping)\n","  merged_df.dropna(subset=['offenseFormation'], inplace=True)\n","\n","  return merged_df\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","def split_data_by_uniqueId(df, train_ratio=0.7, test_ratio=0.15, val_ratio=0.15, unique_id_column=\"uniqueId\"):\n","\n","  \"\"\"\n","  Split the dataframe into training, testing, and validation sets based on a given ratio while\n","  ensuring all rows with the same uniqueId are in the same set.\n","\n","  :param df: the aggregate dataframe containing all frames for each play\n","  :param train_ratio: proportion of the data to allocate to training (default 0.7)\n","  :param test_ratio: proportion of the data to allocate to testing (default 0.15)\n","  :param val_ratio: proportion of the data to allocate to validation (default 0.15)\n","  :param unique_id_column: the name of the column containing the unique identifiers for each play\n","\n","  :return: three dataframes (train_df, test_df, val_df) for training, testing, and validation\n","  \"\"\"\n","\n","  unique_ids = df[unique_id_column].unique()\n","  np.random.shuffle(unique_ids)\n","\n","  num_ids = len(unique_ids)\n","  train_end = int(train_ratio * num_ids)\n","  test_end = train_end + int(test_ratio * num_ids)\n","\n","  train_ids = unique_ids[:train_end]\n","  test_ids = unique_ids[train_end:test_end]\n","  val_ids = unique_ids[test_end:]\n","\n","  train_df = df[df[unique_id_column].isin(train_ids)]\n","  test_df = df[df[unique_id_column].isin(test_ids)]\n","  val_df = df[df[unique_id_column].isin(val_ids)]\n","\n","  print(f\"Train Dataframe Frames: {train_df.shape[0]}\")\n","  print(f\"Test Dataframe Frames: {test_df.shape[0]}\")\n","  print(f\"Val Dataframe Frames: {val_df.shape[0]}\")\n","\n","  return train_df, test_df, val_df\n","\n","\n","def pass_attempt_merging(tracking, plays):\n","\n","  plays['passAttempt'] = np.where(plays['passResult'].isin([np.nan, 'S']), 0, 1)\n","\n","  plays_for_merge = plays[['gameId', 'playId', 'passAttempt']]\n","\n","  merged_df = tracking.merge(\n","      plays_for_merge,\n","      on=['gameId', 'playId'],\n","      how='left')\n","\n","  return merged_df\n","\n","\n","#def prepare_frame_data(df, features, target_column):\n","\n"," # features_array = df.groupby(\"frameUniqueId\")[features].apply(\n","  #    lambda x: x.to_numpy(dtype=np.float32)).to_numpy()\n","\n","#  try:\n","#      features_tensor = torch.tensor(np.stack(features_array))\n","#  except ValueError as e:\n"," #     print(\"Skipping batch due to inconsistent shapes in features_array:\", e)\n"," #     return None, None  # or return some placeholder values if needed\n","\n","#  targets_array = df.groupby(\"frameUniqueId\")[target_column].first().to_numpy()\n"," # targets_tensor = torch.tensor(targets_array, dtype=torch.long)\n","\n"," # return features_tensor, targets_tensor\n","\n","\n","def select_augmented_frames(df, num_samples, sigma=5):\n","\n","    df_frames = df[['frameUniqueId', 'frames_from_snap']].drop_duplicates()\n","    weights = np.exp(-((df_frames['frames_from_snap'] + 10) ** 2) / (2 * sigma ** 2))\n","\n","    weights /= weights.sum()\n","\n","    selected_frames = np.random.choice(\n","        df_frames['frameUniqueId'], size=num_samples, replace=False, p=weights\n","    )\n","\n","    return selected_frames\n","\n","\n","def data_augmentation(df, augmented_frames):\n","\n","  df_sample = df.loc[df['frameUniqueId'].isin(augmented_frames)].copy()\n","\n","  df_sample['y_clean'] = (160 / 3) - df_sample['y_clean']\n","  df_sample['dir_radians'] = (2 * np.pi) - df_sample['dir_radians']\n","  df_sample['dir_clean'] = np.degrees(df_sample['dir_radians'])\n","\n","  df_sample['frameUniqueId'] = df_sample['frameUniqueId'].astype(str) + '_aug'\n","\n","  return df_sample"],"id":"vLIVGV5ZJpW_"},{"cell_type":"code","source":["def add_los_depth_and_angles(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Robustly compute LOS (los_x) per (gameId, playId) using the frame CLOSEST to the snap.\n","    Priority:\n","      1) ball row (club == 'football') at |frames_from_snap| minimum\n","      2) offense rows at that frame: min x_clean (offense moves L->R)\n","      3) fallback: per-play min x_clean\n","    Then add:\n","      - depth_to_los = x_clean - los_x\n","      - o_to_los_cos = cos(dir_clean in radians)\n","    \"\"\"\n","    # ensure we have frames_from_snap and x_clean/y_clean/dir_clean\n","    if \"frames_from_snap\" not in df.columns:\n","        raise KeyError(\"frames_from_snap not found. Compute snap_frame and frames_from_snap before LOS.\")\n","    if \"x_clean\" not in df.columns or \"dir_clean\" not in df.columns:\n","        raise KeyError(\"x_clean/dir_clean not found. Run make_plays_left_to_right and rotate_direction_and_orientation first.\")\n","\n","    # find the frameId closest to snap per play\n","    closest_idx = (\n","        df.loc[:, [\"gameId\",\"playId\",\"frameId\",\"frames_from_snap\"]]\n","          .assign(abs_fs=lambda x: x[\"frames_from_snap\"].abs())\n","          .sort_values([\"gameId\",\"playId\",\"abs_fs\",\"frameId\"])\n","          .groupby([\"gameId\",\"playId\"], as_index=False)\n","          .first()[[\"gameId\",\"playId\",\"frameId\"]]\n","          .rename(columns={\"frameId\": \"closest_frame\"})\n","    )\n","    df = df.merge(closest_idx, on=[\"gameId\",\"playId\"], how=\"left\")\n","\n","    # 1) Try ball at closest frame\n","    ball = (\n","        df[(df[\"club\"] == \"football\") & (df[\"frameId\"] == df[\"closest_frame\"])]\n","        .loc[:, [\"gameId\",\"playId\",\"x_clean\"]]\n","        .drop_duplicates()\n","        .rename(columns={\"x_clean\": \"los_x\"})\n","    )\n","\n","    # 2) Offense at closest frame (defense==0 and not football), take min x_clean\n","    off_at_closest = (\n","        df[(df[\"frameId\"] == df[\"closest_frame\"]) & (df[\"club\"] != \"football\")]\n","    )\n","    if \"defense\" in off_at_closest.columns:\n","        off_at_closest = off_at_closest[off_at_closest[\"defense\"] == 0]\n","\n","    off_min = (\n","        off_at_closest.groupby([\"gameId\",\"playId\"], as_index=False)[\"x_clean\"]\n","        .min()\n","        .rename(columns={\"x_clean\": \"los_x_off\"})\n","    )\n","\n","    # merge LOS candidates\n","    df = df.merge(ball, on=[\"gameId\",\"playId\"], how=\"left\")\n","    df = df.merge(off_min, on=[\"gameId\",\"playId\"], how=\"left\")\n","\n","    # 3) final los_x: prefer ball, else offense min at closest, else per-play min x_clean\n","    per_play_min = (\n","        df.groupby([\"gameId\",\"playId\"], as_index=False)[\"x_clean\"].min()\n","          .rename(columns={\"x_clean\": \"los_x_fallback\"})\n","    )\n","    df = df.merge(per_play_min, on=[\"gameId\",\"playId\"], how=\"left\")\n","\n","    df[\"los_x\"] = df[\"los_x\"].fillna(df[\"los_x_off\"])\n","    df[\"los_x\"] = df[\"los_x\"].fillna(df[\"los_x_fallback\"])\n","    df.drop(columns=[\"los_x_off\",\"los_x_fallback\",\"closest_frame\"], inplace=True, errors=\"ignore\")\n","\n","    # add depth/angle features\n","    df[\"depth_to_los\"] = df[\"x_clean\"] - df[\"los_x\"]\n","    df[\"o_to_los_cos\"] = np.cos(np.radians(df[\"dir_clean\"]))\n","    return df\n"],"metadata":{"id":"PhISufgqeJCS"},"execution_count":null,"outputs":[],"id":"PhISufgqeJCS"},{"cell_type":"code","source":["import os, gc, numpy as np, pandas as pd\n","\n","# Window around snap (tracking is ~10 Hz)\n","HZ = 10\n","T_PRE_S, T_POST_S = 0.8, 0.5\n","T_PRE_FRAMES  = int(T_PRE_S  * HZ)   # 8 frames pre\n","T_POST_FRAMES = int(T_POST_S * HZ)   # 5 frames post\n","\n","def _ensure_distance_column_inplace(df: pd.DataFrame) -> None:\n","    \"\"\"If 'dis' is missing, compute distance moved since previous frame per (gameId, playId, nflId).\"\"\"\n","    if \"dis\" in df.columns:\n","        return\n","    df.sort_values([\"gameId\",\"playId\",\"nflId\",\"frameId\"], inplace=True)\n","    same_entity = (\n","        df[\"gameId\"].diff().eq(0) &\n","        df[\"playId\"].diff().eq(0) &\n","        df[\"nflId\"].diff().eq(0)\n","    )\n","    dx = df[\"x\"].diff()\n","    dy = df[\"y\"].diff()\n","    df[\"dis\"] = np.where(same_entity, np.sqrt(dx*dx + dy*dy), 0.0).astype(\"float64\")"],"metadata":{"id":"PEs80D1Mq-3n"},"execution_count":null,"outputs":[],"id":"PEs80D1Mq-3n"},{"cell_type":"code","source":["CROSS_TMAX_FRAMES = 13   # ~1.3s after snap at 10 Hz\n","DL_POSITIONS = {'DT','NT','DE','EDGE','DI'}  # adjust to your position codes\n","\n","def _first_cross_flags(gdf):\n","    \"\"\"Return set of nflIds that cross LOS within CROSS_TMAX_FRAMES after snap.\"\"\"\n","    snapF = int(gdf['snap_frame'].iloc[0])\n","    los_x = float(gdf['los_x'].iloc[0])\n","    def_team = gdf['defensiveTeam'].iloc[0]\n","    d = gdf[(gdf['club']==def_team) & (gdf['nflId'].notna())].copy()\n","    window = d[(d['frameId'] >= snapF) & (d['frameId'] <= snapF + CROSS_TMAX_FRAMES)]\n","    # Offense is moving L->R; a defender \"rusher\" is someone whose x_clean <= los_x at any time in window\n","    crossed = (window.groupby('nflId')['x_clean'].apply(lambda x: (x <= los_x).any()))\n","    return set(crossed[crossed].index.astype(np.int64))\n","\n","def make_blitz_labels(play_df, players_df=None):\n","    \"\"\"\n","    Returns a play-level labels DataFrame with columns:\n","      blitz (0/1), num_rushers, sim_blitz (0/1)\n","    and merges back to play_df.\n","    \"\"\"\n","    pos_map = {}\n","    if players_df is not None and {'nflId','position'}.issubset(players_df.columns):\n","        pos_map = players_df[['nflId','position']].drop_duplicates().set_index('nflId')['position'].to_dict()\n","\n","    recs = []\n","    for (g,p), gdf in play_df.groupby(['gameId','playId']):\n","        rushers = _first_cross_flags(gdf)\n","        num_rush = len(rushers)\n","        blitz = int(num_rush >= 5)\n","\n","        # creeper/simulated: 4 rushers AND any non-DL rushed AND any DL dropped ≥3y behind LOS in first 0.8s\n","        sim_blitz = 0\n","        if num_rush == 4 and pos_map:\n","            non_dl_rushed = any(pos_map.get(nid, '') not in DL_POSITIONS for nid in rushers)\n","            snapF = int(gdf['snap_frame'].iloc[0])\n","            los_x = float(gdf['los_x'].iloc[0])\n","            dl = gdf[(gdf['club']==gdf['defensiveTeam'].iloc[0]) &\n","                     (gdf['position'].isin(DL_POSITIONS)) &\n","                     (gdf['frameId'] >= snapF) & (gdf['frameId'] <= snapF+8)].copy()\n","            dl['depth'] = dl['x_clean'] - los_x\n","            dl_drop = (dl.groupby('nflId')['depth'].max() >= 3.0).any() if not dl.empty else False\n","            sim_blitz = int(non_dl_rushed and dl_drop)\n","\n","        recs.append((g,p,blitz,num_rush,sim_blitz))\n","\n","    lab = pd.DataFrame(recs, columns=['gameId','playId','blitz','num_rushers','sim_blitz'])\n","    return play_df.merge(lab, on=['gameId','playId'], how='left')\n"],"metadata":{"id":"uQ_2kCBWqQwq"},"execution_count":null,"outputs":[],"id":"uQ_2kCBWqQwq"},{"cell_type":"code","source":["def add_disguise_features(df: pd.DataFrame, pre_window=(-8, 0)) -> pd.DataFrame:\n","    lo, hi = pre_window\n","    pre = df[(df['frames_from_snap'] >= lo) & (df['frames_from_snap'] <= hi)].copy()\n","    if pre.empty:\n","        for c in ['creep_depth_mean','creep_depth_max','creep_lat_mean','creep_lat_max',\n","                  'pre_speed_mean','pre_speed_max','pre_face_cos','pre_depth_mean']:\n","            df[c] = 0.0\n","        return df\n","\n","    pre.sort_values(['gameId','playId','nflId','frameId'], inplace=True)\n","    pre['delta_depth'] = pre.groupby(['gameId','playId','nflId'])['depth_to_los'].diff().fillna(0.0)\n","    pre['delta_lat']   = pre.groupby(['gameId','playId','nflId'])['y_clean'].diff().fillna(0.0)\n","\n","    agg = pre.groupby(['gameId','playId','nflId']).agg(\n","        creep_depth_mean=('delta_depth','mean'),\n","        creep_depth_max =('delta_depth','max'),\n","        creep_lat_mean  =('delta_lat','mean'),\n","        creep_lat_max   =('delta_lat','max'),\n","        pre_speed_mean  =('s_clean','mean') if 's_clean' in pre.columns else ('s','mean'),\n","        pre_speed_max   =('s_clean','max') if 's_clean' in pre.columns else ('s','max'),\n","        pre_face_cos    =('o_to_los_cos','mean'),\n","        pre_depth_mean  =('depth_to_los','mean'),\n","    ).reset_index()\n","\n","    return df.merge(agg, on=['gameId','playId','nflId'], how='left')"],"metadata":{"id":"_5LKdxiarEfg"},"execution_count":null,"outputs":[],"id":"_5LKdxiarEfg"},{"cell_type":"code","source":["import torch\n","\n","def _threat_score(sub):\n","    # nearer LOS (smaller |depth|), moving toward LOS (+cos), faster\n","    return -np.abs(sub['depth_to_los']) + 0.4*sub['s_clean'] + 0.2*sub['o_to_los_cos']\n","\n","def prepare_frame_data_blitz(df, features, target_column, K=8):\n","    \"\"\"\n","    Builds tensors:\n","      X: [N_frames, K, F]  (defense-only, top-K by threat, zero-padded)\n","      y: [N_frames]        (play-level blitz label broadcast to frames)\n","    \"\"\"\n","    # Defense only and not the ball\n","    df_def = df[(df['defense']==1) & (df['club']!='football')].copy()\n","\n","    # play-level labels once per frameUniqueId\n","    y_map = df_def.groupby('frameUniqueId')[target_column].first().astype(int).to_dict()\n","\n","    X_list, y_list = [], []\n","    for fuid, sub in df_def.groupby('frameUniqueId'):\n","        sub = sub.copy()\n","        # robust: if any feature missing, fill 0\n","        for c in features:\n","            if c not in sub.columns:\n","                sub[c] = 0.0\n","\n","        # rank by threat within this frame\n","        sub['threat'] = _threat_score(sub)\n","        sub.sort_values('threat', ascending=False, inplace=True)\n","        mat = sub[features].to_numpy(dtype=np.float32)\n","\n","        # pad/truncate to K\n","        if mat.shape[0] >= K:\n","            matK = mat[:K]\n","        else:\n","            pad = np.zeros((K - mat.shape[0], mat.shape[1]), dtype=np.float32)\n","            matK = np.vstack([mat, pad])\n","\n","        X_list.append(matK)\n","        y_list.append(y_map.get(fuid, 0))\n","\n","    if not X_list:\n","        return None, None\n","\n","    X = torch.tensor(np.stack(X_list))   # [N, K, F]\n","    y = torch.tensor(np.array(y_list), dtype=torch.long)\n","    return X, y"],"metadata":{"id":"bjSmtqPtshnB"},"execution_count":null,"outputs":[],"id":"bjSmtqPtshnB"}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}